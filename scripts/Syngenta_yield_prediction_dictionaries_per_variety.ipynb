{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reasons for this notebook:\n",
    "# The main reason to have a dictionary approach like this is to have a dictionary for each\n",
    "# variety that will have a classifier that is specific to that variety. In this case, 174 varieties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THERE ARE SEPARATE NOTEBOOKS FOR VISUALIZATIONS, DATASET ANALYSIS, ETC. IN THE REPO.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# READ THE CSV INTO DATAFRAME\n",
    "\n",
    "df = pd.read_csv('Syngenta/Syngenta_2017/Experiment_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GOAL OF THIS MODULE:\n",
    "# Encode the planting date as a season\n",
    "\n",
    "# remove the dates that are \".\"\n",
    "df = df[~df['Planting date'].str.match(\"\\.\")]\n",
    "plant_date = df['Planting date'].apply(lambda dt: pd.to_datetime(dt))\n",
    "plant_months = plant_date.apply(lambda dt: dt.month)\n",
    "season = plant_date.rename(\"Season\")\n",
    "season = pd.to_datetime(season)\n",
    "season = season.apply(lambda dt: (dt.month%12 + 3)//3)\n",
    "# df['Plant date'] = pd.to_datetime(df['Plant date'])\n",
    "df = pd.concat([df, season], axis=1)\n",
    "\n",
    "# plant_date = pd.to_datetime(df['Planting date'], infer_datetime_format=True)\n",
    "# df = df['Planting date'].apply(lambda dt: (dt.month%12 + 3)//3)\n",
    "# pd.get_dummies(df['Planting date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD MONTH OF MAY AND JUNE ONE HOT ENCODING INTO THE DATAFRAME\n",
    "pd.get_dummies(plant_months).sum()\n",
    "june = pd.get_dummies(plant_months).loc[:,6]\n",
    "june = june.rename(\"June\")\n",
    "may = pd.get_dummies(plant_months).loc[:,5]\n",
    "may = may.rename(\"May\")\n",
    "df = pd.concat([df, may], axis=1)\n",
    "df = pd.concat([df, june], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LATITUDE AND LONGITUDE CLUSTERING INTO FEATURES\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "latlong = df.loc[:, ['Latitude', 'Longitude']]\n",
    "\n",
    "kmeans = KMeans(n_clusters=4, random_state=0, n_jobs=-1).fit(latlong)\n",
    "kmeans.labels_.shape\n",
    "lat_long_dummies = pd.get_dummies(kmeans.labels_)\n",
    "lat_long_dummies = lat_long_dummies.rename(index=int, columns={0: \"Loc Clust 0\",\n",
    "                                                               1: \"Loc Clust 1\",\n",
    "                                                               2: \"Loc Clust 2\",\n",
    "                                                               3: \"Loc Clust 3\"})\n",
    "df = pd.concat([df, lat_long_dummies], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#REMOVE ANY NAN VALUES\n",
    "\n",
    "print(df.columns)\n",
    "df = df[~df.Silt.isnull()]\n",
    "df = df[~df['Loc Clust 1'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DROP ALL THE CELLS THAT ARE NOT USABLE SUCH AS THE ONES THAT ARE STRINGS OR DATES\n",
    "\n",
    "# set if want to drop some columns specifically\n",
    "should_drop = 1\n",
    "# columns_to_drop = ['Experiment', 'Location',\n",
    "#                    'Check Yield', 'Yield difference', 'Latitude',\n",
    "#                    'Longitude', 'PI', 'Variety', 'Planting date', 'Season']\n",
    "\n",
    "# BELOW DROP IS USED FOR THE DF_DICT APPROACH\n",
    "columns_to_drop = ['Experiment', 'Location',\n",
    "                   'Check Yield', 'Yield difference', 'PI', 'Planting date', 'Season']\n",
    "\n",
    "# set if want to keep some columns specifically\n",
    "should_keep = 0\n",
    "# columns_to_keep = ['Loc Clust 0', 'Loc Clust 1', 'Loc Clust 2', 'Loc Clust 3']\n",
    "columns_to_keep_top = ['Silt', 'Precipitation', 'Temperature', 'Solar Radiation', 'Organic matter']\n",
    "# columns_VARIETIES_ONLY = np.asarray(df.iloc[:, df.columns.str.match('V\\d\\d\\d\\d\\d\\d')].columns)\n",
    "\n",
    "#set the below variable to whatever columns you want to keep\n",
    "columns_to_keep = columns_to_keep_top\n",
    "\n",
    "MUST_HAVE_COLUMNS = ['Yield']\n",
    "# print(columns_to_keep)\n",
    "\n",
    "df = df.drop(columns_to_drop, axis=1) if should_drop else df\n",
    "df = df.loc[:, np.concatenate((columns_to_keep, MUST_HAVE_COLUMNS))] if should_keep else df\n",
    "df['YieldBucket'] = pd.Series(pd.qcut(df.Yield, q=3, labels=[\"high\", \"medium\", \"low\"]))\n",
    "print(\"The final dataframe has columns: \", df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LET US ALSO MAKE SURE THERE ARE NO NAN IN THE DATA\n",
    "\n",
    "print(\"We expect to be %s nan values and there actually are %s nan values\\n\" % (0, np.sum(df.isnull().sum())))\n",
    "print(df.isnull().sum())\n",
    "# AFTER COLUMNS, MAKE SURE NO SKETCHY ONES\n",
    "for col in df.columns:\n",
    "    print(col, type(df[col][0]))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GOAL OF THIS MODULE:\n",
    "# CREATE A DICTIONARY OF DATAFRAMES CONTAINING EACH VARIETY AND CORRESPONDING ROWS\n",
    "# MOST LIKELY SHOULD DO THIS AFTER ADDING AND REMOVING THE DESIRED COLUMNS\n",
    "\n",
    "UNIQUE_VARIETIES = np.unique(df.Variety)\n",
    "df_dict = {variety: df.loc[df.Variety == variety] for variety in UNIQUE_VARIETIES}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict['V000016']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GOAL OF THIS MODULE\n",
    "# ALONG WITH THE PREVIOUS MODULE, CREATE DICTIONARIES FOR EACH OF THE TRAINING AND TEST SETS, SO 4 DICTS TOTAL\n",
    "# WHICH EACH CONTAIN A DISTINCT TRAINING AND TEST SET FOR EACH VARIETY\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def cross_validation_split(data):\n",
    "    \n",
    "    \n",
    "    if type(data) == dict:\n",
    "        X_train_dict = {}\n",
    "        X_test_dict = {}\n",
    "        y_train_dict = {}\n",
    "        y_test_dict = {}\n",
    "        for variety, dataf in data.items():\n",
    "            X = dataf.drop(['Yield', 'YieldBucket', 'Variety'], axis=1)\n",
    "            y = dataf.Yield\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.05, train_size = 0.95,\n",
    "                                                                random_state = 42)\n",
    "            \n",
    "            X_train_dict[variety] = X_train\n",
    "            X_test_dict[variety] = X_test\n",
    "            y_train_dict[variety] = y_train\n",
    "            y_test_dict[variety] = y_test\n",
    "            \n",
    "        return X_train_dict, X_test_dict, y_train_dict, y_test_dict\n",
    "#     else:\n",
    "#         # ADD BELOW MODULE HERE\n",
    "            \n",
    "            \n",
    "X_train_dict, X_test_dict, y_train_dict, y_test_dict = cross_validation_split(df_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before = np.array([])\n",
    "beforev = np.array([])\n",
    "for key, value in X_test_dict.items():\n",
    "    before = np.append(before, value.shape[0])\n",
    "    beforev = np.append(beforev, key)\n",
    "    \n",
    "pd.DataFrame({\n",
    "    \"variety\": beforev,\n",
    "    \"count\": before\n",
    "}).sort_values(by=\"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_dict['V000016']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS FUNCTION WILL EVALUATE ERRORS BASED ON RMSE (FROM SYNGENTA CHALLENGE SPEC)\n",
    "# AND ALSO WILL EVALUATE BASED ON AVERAGE ERROR\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def evaluate_errors(prediction, actual):\n",
    "    RMSE_error = np.sqrt(mean_squared_error(prediction, actual))\n",
    "    print(\"RMSE Error: \", np.sqrt(mean_squared_error(prediction, actual)))\n",
    "    avg_error_vector = np.absolute(((prediction - actual) / actual) * 100)\n",
    "    print(\"Average Error details:\\n\", avg_error_vector.describe())\n",
    "    return avg_error_vector, RMSE_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET OUTPUT OF FEATURE IMPORTANCE\n",
    "\n",
    "def get_feature_importances(regr):\n",
    "    feature_importances = regr.feature_importances_\n",
    "    feature_importances = pd.Series(feature_importances)\n",
    "    feature_importance_df = pd.DataFrame({'feature': X_train.columns,'feature_importance': feature_importances})\n",
    "    feature_importance_df = feature_importance_df.sort_values(by=['feature_importance'])\n",
    "    for index, row in feature_importance_df.iterrows():\n",
    "        print(row['feature'], 'has importance: ', row['feature_importance'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS IS TO BE USED WITH THE DICTIONARY APPROACH ONLY\n",
    "\n",
    "from sklearn.base import clone\n",
    "\n",
    "def train_on_varieties(clf, X_train_dict_, y_train_dict_):\n",
    "    clfs = {}\n",
    "    for variety, dataf in X_train_dict_.items():\n",
    "        clf = clone(clf)\n",
    "        clf.fit(X_train_dict_[variety], y_train_dict[variety])\n",
    "        clfs[variety] = clf\n",
    "    return clfs\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "clf = RandomForestRegressor(n_estimators=, max_depth=13, random_state=0, verbose=1, n_jobs=-1)\n",
    "\n",
    "clfs = train_on_varieties(clf, X_train_dict, y_train_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS IS USED FOR TESTING WITH THE DICTIONARY APPROACH\n",
    "\n",
    "def test_on_varieties(classifier_dict, X_test_dict_, y_test_dict_):\n",
    "    preds = {}\n",
    "    accuracies = {}\n",
    "    for variety, classifier in classifier_dict.items():\n",
    "        preds[variety] = classifier.predict(X_test_dict_[variety])\n",
    "        accuracies[variety] = evaluate_errors(preds[variety], y_test_dict_[variety])\n",
    "    return preds, accuracies\n",
    "        \n",
    "preds, accuracies = test_on_varieties(clfs, X_test_dict, y_test_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS IS SOME ANALYSIS OF THE PREDICTIONS OF THE DICTIONARY APPROACH\n",
    "\n",
    "# for var, accs in accuracies.items():\n",
    "#     print(accs)\n",
    "# #     print(var, accs[0].describe())\n",
    "#     all_means = np.array([])\n",
    "#     np.append(all_means, accs[0].describe()['mean'])\n",
    "# #     print(accs[0].describe()['mean'])\n",
    "#     print(all_means.shape)\n",
    "all_means = np.array([])\n",
    "all_RMSE = np.array([])\n",
    "varieties = np.array([])\n",
    "\n",
    "for variety, accuracy_tuple in accuracies.items():\n",
    "    all_means = np.append(all_means, accuracy_tuple[0].describe()['mean'])\n",
    "    all_RMSE = np.append(all_RMSE, accuracy_tuple[1])\n",
    "    varieties = np.append(varieties,variety)\n",
    "# accuracies['V000016'][0].describe()['mean']\n",
    "# print(all_means.shape)\n",
    "import pprint\n",
    "from scipy.stats import describe\n",
    "pprint.pprint(describe(all_means))\n",
    "np.mean(all_means)\n",
    "# print(all_means)\n",
    "print(np.mean(all_RMSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varieties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE_df = pd.DataFrame(all_RMSE)\n",
    "varieties_df = pd.DataFrame(varieties)\n",
    "# numbers_varieties_df = pd.DataFrame([df_dict[var].shape[0] for var in varieties_df])\n",
    "numbers_varieties_df = pd.DataFrame([df_dict[var].shape[0] for idx,var in varieties_df[0].items()])\n",
    "RMSE_df = pd.concat([RMSE_df, varieties_df], axis=1)\n",
    "RMSE_df = pd.concat([RMSE_df, numbers_varieties_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSEP_df = pd.DataFrame(RMSE_df.RMSE / pd.DataFrame([df_dict[var]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE_df = RMSE_df.loc[:, [\"RMSE\", \"VAR\", \"COUNT\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_yields_df = pd.DataFrame([df_dict[var].Yield.describe()['mean'] for idx,var in varieties_df[0].items()])\n",
    "RMSE_df = pd.concat([RMSE_df, RMSE_df.RMSE / mean_yields_df[0]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE_df.sort_values(by=0).RMSE.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE_df.columns = ['RMSE', 'VAR', 'COUNT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE_df = RMSE_df.sort_values(by=['RMSE'])\n",
    "RMSE_df\n",
    "np.corrcoef(RMSE_df.RMSE, RMSE_df.COUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, var in RMSE_df.iterrows():\n",
    "    print(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE_df.sort_values(by=\"RMSE\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
