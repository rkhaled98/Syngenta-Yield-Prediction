{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# READ THE CSV INTO DATAFRAME\n",
    "\n",
    "df = pd.read_csv('Syngenta/Syngenta_2017/Experiment_dataset.csv')\n",
    "# df2 = pd.read_csv('Syngenta/Syngenta_2017/Region_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEE SOME OPTIONAL INFORMATION ABOUT THE DATAFRAME\n",
    "\n",
    "# print(df.head())\n",
    "# print(df.describe())\n",
    "print(df)\n",
    "print(df.Variety.unique(), \"\\nthere are \", len(df.Variety.unique()), \" unique varieties\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Yield', 'Year', 'Temperature', 'Precipitation', 'Solar Radiation',\n",
      "       'Soil class', 'CEC', 'Organic matter', 'pH', 'Clay', 'Silt', 'Sand',\n",
      "       'Area'],\n",
      "      dtype='object')\n",
      "             0    1         2         3         4         5         6   \\\n",
      "0      0.457460  0.0  0.361879  0.577475  0.410389  0.049550  0.811016   \n",
      "1      0.441596  0.0  0.361879  0.577475  0.410389  0.049550  0.811016   \n",
      "2      0.402939  0.0  0.361879  0.577475  0.410389  0.049550  0.811016   \n",
      "3      0.435585  0.0  0.361879  0.577475  0.410389  0.049550  0.811016   \n",
      "4      0.364699  0.0  0.361879  0.577475  0.410389  0.049550  0.811016   \n",
      "5      0.388912  0.0  0.361879  0.577475  0.410389  0.049550  0.811016   \n",
      "6      0.403440  0.0  0.361879  0.577475  0.410389  0.049550  0.811016   \n",
      "7      0.354012  0.0  0.361879  0.577475  0.410389  0.049550  0.811016   \n",
      "8      0.418803  0.0  0.361879  0.577475  0.410389  0.049550  0.811016   \n",
      "9      0.421558  0.0  0.361879  0.577475  0.410389  0.049550  0.811016   \n",
      "10     0.585789  0.0  0.467931  0.286548  0.196814  0.459459  0.361722   \n",
      "11     0.540870  0.0  0.467931  0.286548  0.196814  0.459459  0.361722   \n",
      "12     0.567838  0.0  0.467931  0.286548  0.196814  0.459459  0.361722   \n",
      "13     0.617183  0.0  0.467931  0.286548  0.196814  0.459459  0.361722   \n",
      "14     0.400601  0.0  0.361879  0.577475  0.410389  0.049550  0.811016   \n",
      "15     0.368874  0.0  0.361879  0.577475  0.410389  0.049550  0.811016   \n",
      "16     0.416465  0.0  0.361879  0.577475  0.410389  0.049550  0.811016   \n",
      "17     0.428571  0.0  0.361879  0.577475  0.410389  0.049550  0.811016   \n",
      "18     0.585789  0.0  0.467931  0.286548  0.196814  0.459459  0.361722   \n",
      "19     0.545379  0.0  0.467931  0.286548  0.196814  0.459459  0.361722   \n",
      "20     0.549887  0.0  0.467931  0.286548  0.196814  0.459459  0.361722   \n",
      "21     0.594723  0.0  0.467931  0.286548  0.196814  0.459459  0.361722   \n",
      "22     0.527428  0.0  0.467931  0.286548  0.196814  0.459459  0.361722   \n",
      "23     0.576772  0.0  0.467931  0.286548  0.196814  0.459459  0.361722   \n",
      "24     0.581281  0.0  0.467931  0.286548  0.196814  0.459459  0.361722   \n",
      "25     0.398347  0.0  0.413783  0.494527  0.349513  0.009009  0.642077   \n",
      "26     0.351173  0.0  0.413783  0.494527  0.349513  0.009009  0.642077   \n",
      "27     0.540870  0.0  0.467931  0.286548  0.196814  0.459459  0.361722   \n",
      "28     0.540870  0.0  0.467931  0.286548  0.196814  0.459459  0.361722   \n",
      "29     0.581281  0.0  0.467931  0.286548  0.196814  0.459459  0.361722   \n",
      "...         ...  ...       ...       ...       ...       ...       ...   \n",
      "82006  0.486683  1.0  0.753547  0.615057  0.158504  0.459459  0.245134   \n",
      "82007  0.493613  1.0  0.753547  0.615057  0.158504  0.459459  0.245134   \n",
      "82008  0.441680  1.0  0.753547  0.615057  0.158504  0.459459  0.245134   \n",
      "82009  0.456375  1.0  0.753547  0.615057  0.158504  0.459459  0.245134   \n",
      "82010  0.482675  1.0  0.454973  0.574921  0.049593  0.049550  0.347931   \n",
      "82011  0.445270  1.0  0.454973  0.574921  0.049593  0.049550  0.347931   \n",
      "82012  0.472238  1.0  0.454973  0.574921  0.049593  0.049550  0.347931   \n",
      "82013  0.471070  1.0  0.454973  0.574921  0.049593  0.049550  0.347931   \n",
      "82014  0.488854  1.0  0.454973  0.574921  0.049593  0.049550  0.347931   \n",
      "82015  0.404609  1.0  0.454973  0.574921  0.049593  0.049550  0.347931   \n",
      "82016  0.576271  1.0  0.682460  0.460472  0.198329  0.459459  0.578606   \n",
      "82017  0.539618  1.0  0.682460  0.460472  0.198329  0.459459  0.578606   \n",
      "82018  0.545546  1.0  0.682460  0.460472  0.198329  0.459459  0.578606   \n",
      "82019  0.531853  1.0  0.682460  0.460472  0.198329  0.459459  0.578606   \n",
      "82020  0.541705  1.0  0.682460  0.460472  0.198329  0.459459  0.578606   \n",
      "82021  0.463305  1.0  0.753547  0.615057  0.158504  0.459459  0.245134   \n",
      "82022  0.478918  1.0  0.753547  0.615057  0.158504  0.459459  0.245134   \n",
      "82023  0.507473  1.0  0.753547  0.615057  0.158504  0.459459  0.245134   \n",
      "82024  0.492694  1.0  0.753547  0.615057  0.158504  0.459459  0.245134   \n",
      "82025  0.485764  1.0  0.753547  0.615057  0.158504  0.459459  0.245134   \n",
      "82026  0.486432  1.0  0.682460  0.460472  0.198329  0.459459  0.578606   \n",
      "82027  0.477165  1.0  0.682460  0.460472  0.198329  0.459459  0.578606   \n",
      "82028  0.544043  1.0  0.682460  0.460472  0.198329  0.459459  0.578606   \n",
      "82029  0.514319  1.0  0.682460  0.460472  0.198329  0.459459  0.578606   \n",
      "82030  0.608834  1.0  0.682460  0.460472  0.198329  0.459459  0.578606   \n",
      "82031  0.445103  1.0  0.753547  0.615057  0.158504  0.459459  0.245134   \n",
      "82032  0.426902  1.0  0.753547  0.615057  0.158504  0.459459  0.245134   \n",
      "82033  0.491024  1.0  0.753547  0.615057  0.158504  0.459459  0.245134   \n",
      "82034  0.456375  1.0  0.753547  0.615057  0.158504  0.459459  0.245134   \n",
      "82035  0.452868  1.0  0.753547  0.615057  0.158504  0.459459  0.245134   \n",
      "\n",
      "             7         8         9         10        11        12  \n",
      "0      0.203557  0.499232  0.461144  0.328767  0.594016  0.323134  \n",
      "1      0.203557  0.499232  0.461144  0.328767  0.594016  0.323134  \n",
      "2      0.203557  0.499232  0.461144  0.328767  0.594016  0.323134  \n",
      "3      0.203557  0.499232  0.461144  0.328767  0.594016  0.323134  \n",
      "4      0.203557  0.499232  0.461144  0.328767  0.594016  0.323134  \n",
      "5      0.203557  0.499232  0.461144  0.328767  0.594016  0.323134  \n",
      "6      0.203557  0.499232  0.461144  0.328767  0.594016  0.323134  \n",
      "7      0.203557  0.499232  0.461144  0.328767  0.594016  0.323134  \n",
      "8      0.203557  0.499232  0.461144  0.328767  0.594016  0.323134  \n",
      "9      0.203557  0.499232  0.461144  0.328767  0.594016  0.323134  \n",
      "10     0.229585  0.450144  0.160675  0.272692  0.758996  0.642321  \n",
      "11     0.229585  0.450144  0.160675  0.272692  0.758996  0.642321  \n",
      "12     0.229585  0.450144  0.160675  0.272692  0.758996  0.642321  \n",
      "13     0.229585  0.450144  0.160675  0.272692  0.758996  0.642321  \n",
      "14     0.203557  0.499232  0.461144  0.328767  0.594016  0.323134  \n",
      "15     0.203557  0.499232  0.461144  0.328767  0.594016  0.323134  \n",
      "16     0.203557  0.499232  0.461144  0.328767  0.594016  0.323134  \n",
      "17     0.203557  0.499232  0.461144  0.328767  0.594016  0.323134  \n",
      "18     0.229585  0.450144  0.160675  0.272692  0.758996  0.642321  \n",
      "19     0.229585  0.450144  0.160675  0.272692  0.758996  0.642321  \n",
      "20     0.229585  0.450144  0.160675  0.272692  0.758996  0.642321  \n",
      "21     0.229585  0.450144  0.160675  0.272692  0.758996  0.642321  \n",
      "22     0.229585  0.450144  0.160675  0.272692  0.758996  0.642321  \n",
      "23     0.229585  0.450144  0.160675  0.272692  0.758996  0.642321  \n",
      "24     0.229585  0.450144  0.160675  0.272692  0.758996  0.642321  \n",
      "25     0.189762  0.408568  0.511745  0.752372  0.212336  0.466579  \n",
      "26     0.189762  0.408568  0.511745  0.752372  0.212336  0.466579  \n",
      "27     0.229585  0.450144  0.160675  0.272692  0.758996  0.642321  \n",
      "28     0.229585  0.450144  0.160675  0.272692  0.758996  0.642321  \n",
      "29     0.229585  0.450144  0.160675  0.272692  0.758996  0.642321  \n",
      "...         ...       ...       ...       ...       ...       ...  \n",
      "82006  0.147740  0.381352  0.175201  0.713183  0.377305  0.587495  \n",
      "82007  0.147740  0.381352  0.175201  0.713183  0.377305  0.587495  \n",
      "82008  0.147740  0.381352  0.175201  0.713183  0.377305  0.587495  \n",
      "82009  0.147740  0.381352  0.175201  0.713183  0.377305  0.587495  \n",
      "82010  0.236557  0.521504  0.361176  0.461770  0.520941  0.702967  \n",
      "82011  0.236557  0.521504  0.361176  0.461770  0.520941  0.702967  \n",
      "82012  0.236557  0.521504  0.361176  0.461770  0.520941  0.702967  \n",
      "82013  0.236557  0.521504  0.361176  0.461770  0.520941  0.702967  \n",
      "82014  0.236557  0.521504  0.361176  0.461770  0.520941  0.702967  \n",
      "82015  0.236557  0.521504  0.361176  0.461770  0.520941  0.702967  \n",
      "82016  0.166117  0.360359  0.435103  0.932380  0.086989  0.795344  \n",
      "82017  0.166117  0.360359  0.435103  0.932380  0.086989  0.795344  \n",
      "82018  0.166117  0.360359  0.435103  0.932380  0.086989  0.795344  \n",
      "82019  0.166117  0.360359  0.435103  0.932380  0.086989  0.795344  \n",
      "82020  0.166117  0.360359  0.435103  0.932380  0.086989  0.795344  \n",
      "82021  0.147740  0.381352  0.175201  0.713183  0.377305  0.587495  \n",
      "82022  0.147740  0.381352  0.175201  0.713183  0.377305  0.587495  \n",
      "82023  0.147740  0.381352  0.175201  0.713183  0.377305  0.587495  \n",
      "82024  0.147740  0.381352  0.175201  0.713183  0.377305  0.587495  \n",
      "82025  0.147740  0.381352  0.175201  0.713183  0.377305  0.587495  \n",
      "82026  0.166117  0.360359  0.435103  0.932380  0.086989  0.795344  \n",
      "82027  0.166117  0.360359  0.435103  0.932380  0.086989  0.795344  \n",
      "82028  0.166117  0.360359  0.435103  0.932380  0.086989  0.795344  \n",
      "82029  0.166117  0.360359  0.435103  0.932380  0.086989  0.795344  \n",
      "82030  0.166117  0.360359  0.435103  0.932380  0.086989  0.795344  \n",
      "82031  0.147740  0.381352  0.175201  0.713183  0.377305  0.587495  \n",
      "82032  0.147740  0.381352  0.175201  0.713183  0.377305  0.587495  \n",
      "82033  0.147740  0.381352  0.175201  0.713183  0.377305  0.587495  \n",
      "82034  0.147740  0.381352  0.175201  0.713183  0.377305  0.587495  \n",
      "82035  0.147740  0.381352  0.175201  0.713183  0.377305  0.587495  \n",
      "\n",
      "[82036 rows x 13 columns]\n",
      "Yield has correlation [2438155.42564917] with yield\n",
      "Year has correlation [3260451.29499999] with yield\n",
      "Temperature has correlation [2160759.30634827] with yield\n",
      "Precipitation has correlation [2151187.69218195] with yield\n",
      "Solar Radiation has correlation [1608548.11212687] with yield\n",
      "Soil class has correlation [1530292.0245946] with yield\n",
      "CEC has correlation [2922193.25387557] with yield\n",
      "Organic matter has correlation [1384113.87219878] with yield\n",
      "pH has correlation [2526786.97093114] with yield\n",
      "Clay has correlation [2029012.02539768] with yield\n",
      "Silt has correlation [2513717.4553957] with yield\n",
      "Sand has correlation [2162157.87827142] with yield\n",
      "Area has correlation [2643209.70050482] with yield\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kafi/.local/lib/python3.5/site-packages/ipykernel_launcher.py:24: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n"
     ]
    }
   ],
   "source": [
    "# OPTIONAL CORRELATION CHECKER\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# x = df.values #returns a numpy array\n",
    "# min_max_scaler = preprocessing.MinMaxScaler()\n",
    "# x_scaled = min_max_scaler.fit_transform(x)\n",
    "# df = pandas.DataFrame(x_scaled)\n",
    "\n",
    "#these were selected mainly due to their dtype\n",
    "df_potential_columns = df.drop(['Experiment', 'Location', 'Planting date', 'Check Yield', 'Yield difference', 'Latitude', 'Longitude', 'Variety', 'PI'], axis=1).columns\n",
    "# potential_feature_columns = ['Location', 'Temperature', 'Precipitation', 'Solar Radiation', 'Soil class', 'CEC', 'pH', 'Clay', 'Silt', 'Sand', 'Area']\n",
    "# print(potential_feature_columns)\n",
    "potential_output_column = df.Yield\n",
    "\n",
    "new_df = df.loc[:, df_potential_columns]\n",
    "print(new_df.columns)\n",
    "\n",
    "potential_X = df.loc[:, df_potential_columns].values\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "# normalize = preprocessing.normalize()\n",
    "potential_y = potential_output_column\n",
    "x_scaled = min_max_scaler.fit_transform(potential_X)\n",
    "y_scaled = preprocessing.normalize(potential_y.reshape(-1,1))\n",
    "df2 = pd.DataFrame(x_scaled)\n",
    "print(df2)\n",
    "\n",
    "for x in df2:\n",
    "    print(\"%s has correlation %s with yield\" % (df.loc[:, df_potential_columns].iloc[:, x].name, np.correlate(df2[x], potential_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREVIOUSLY USED ONEHOT ENCODER\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from pandas import Series\n",
    "le = LabelEncoder()\n",
    "integer_encoded = le.fit_transform(df.Variety)\n",
    "# le.classes_\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "\n",
    "print(onehot_encoded)\n",
    "\n",
    "# df['VarietyEncode'] = onehot_encoded\n",
    "# print(pd.get_dummies(df.Variety))\n",
    "# print(df.Variety)\n",
    "    \n",
    "from numpy import argmax\n",
    "inverted = le.inverse_transform([argmax(onehot_encoded[300, :])])\n",
    "print([argmax(onehot_encoded[300, :])])\n",
    "verted = le.transform(['V150834'])\n",
    "print(inverted)\n",
    "# inverted = le.inverse_transform([df.])\n",
    "# print(inverted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL USED FROM CONVERTING EACH VARIETY TO A ONE HOT\n",
    "\n",
    "def convert_variety_to_one_hot(variety):\n",
    "    return onehot_encoded[le.transform([variety])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL APPLY ABOVE FUNCTION TO REPLACE EACH ROW IN VARIETY COLUMN WITH ONE HOT REPRESENTATION\n",
    "\n",
    "df.Variety = df.Variety.apply(lambda val: convert_variety_to_one_hot(val))\n",
    "\n",
    "# OPTIONAL RESHAPE\n",
    "df.Variety = df.Variety.apply(lambda val: val.reshape(174))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CURRENTLY NECESSARY IF: USING 174 ADDITIONAL VARIETY COLUMNS METHOD\n",
    "\n",
    "# THIS IS A DIFFERENT APPROACH TO THE ABOVE FOUR CELLS, WHERE WE HAVE 174 ADDITIONAL FEATURE COLUMNS\n",
    "# EACH WITH A 0 (IF IT IS NOT OF THAT VARIETY) OR A 1 (IF IT IS OF THAT VARIETY)\n",
    "\n",
    "# print(df)\n",
    "dummies = pd.get_dummies(df.Variety)\n",
    "# print(dummies)\n",
    "df = pd.concat([df, dummies], axis=1)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ORIGINAL COLUMNS! WE WANT TO PICK ONES THAT ARE GOOD FOR OUR LEARNING ALGORITHM AND DROP THE REST IN THE\n",
    "# FOLLOWING CELL\n",
    "\n",
    "for col in df.columns:\n",
    "    print(col, type(df[col][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DROP ALL THE CELLS THAT ARE NOT USABLE SUCH AS THE ONES THAT ARE STRINGS OR DATES\n",
    "\n",
    "df = df.drop(['Experiment', 'Location', 'Planting date', 'Check Yield', 'Yield difference', 'Latitude', 'Longitude', 'Variety', 'PI'], axis=1)\n",
    "df['YieldBucket'] = pd.Series(pd.qcut(df.Yield, q=3, labels=[\"high\", \"medium\", \"low\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.YieldBucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We expect to be 0 nan values and there actually are 0 nan values\n",
      "\n",
      "Yield <class 'numpy.float64'>\n",
      "Year <class 'numpy.int64'>\n",
      "Temperature <class 'numpy.float64'>\n",
      "Precipitation <class 'numpy.float64'>\n",
      "Solar Radiation <class 'numpy.int64'>\n",
      "Soil class <class 'numpy.int64'>\n",
      "CEC <class 'numpy.float64'>\n",
      "Organic matter <class 'numpy.float64'>\n",
      "pH <class 'numpy.float64'>\n",
      "Clay <class 'numpy.float64'>\n",
      "Silt <class 'numpy.float64'>\n",
      "Sand <class 'numpy.float64'>\n",
      "Area <class 'numpy.float64'>\n",
      "YieldBucket <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "# LET US ALSO MAKE SURE THERE ARE NO NAN IN THE DATA\n",
    "print(\"We expect to be %s nan values and there actually are %s nan values\\n\" % (0, np.sum(df.isnull().sum())))\n",
    "# AFTER COLUMNS, MAKE SURE NO SKETCHY ONES\n",
    "for col in df.columns:\n",
    "    print(col, type(df[col][0]))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(dataframe):\n",
    "    potential = dataframe.values\n",
    "    min_max_scaler = preprocessing.StandardScaler()\n",
    "    x_scaled = min_max_scaler.fit_transform(potential)\n",
    "    return pd.DataFrame(x_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Year', 'Temperature', 'Precipitation', 'Solar Radiation', 'Soil class',\n",
      "       'CEC', 'Organic matter', 'pH', 'Clay', 'Silt', 'Sand', 'Area'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# TRAIN AND TEST SPLIT# TRAIN AND TEST SPLIT# TRAIN AND TEST SPLIT\n",
    "# TRAIN AND TEST SPLIT# TRAIN AND TEST SPLIT# TRAIN AND TEST SPLIT\n",
    "# TRAIN AND TEST SPLIT# TRAIN AND TEST SPLIT# TRAIN AND TEST SPLIT\n",
    "# TRAIN AND TEST SPLIT# TRAIN AND TEST SPLIT# TRAIN AND TEST SPLIT\n",
    "\n",
    "# feature_columns = ['Variety', 'Solar Radiation', 'Temperature', 'Precipitation', 'Location', 'Clay', 'Silt', 'Sand', 'pH', 'Precipitation', 'CEC', 'Soil class']\n",
    "\n",
    "X = df.drop(['Yield', 'YieldBucket'], axis=1)\n",
    "\n",
    "print(X.columns)\n",
    "# X = df.loc[:, feature_columns]\n",
    "\n",
    "y = df.Yield\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.05, train_size = 0.1, random_state = 42)\n",
    "\n",
    "# train_visual, zz, zzz, zzzz = train_test_split(df, y, test_size=0.2, train_size=0.8, random_state = 42)\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "estimator = RandomForestRegressor(n_estimators=10, max_depth=20, random_state=0)\n",
    "# estimator = SVR(kernel=\"linear\")\n",
    "# estimator = \n",
    "selector = RFECV(estimator, step=1, cv=5, verbose=1)\n",
    "selector = selector.fit(X_train, y_train)\n",
    "selector.support_ \n",
    "selector.ranking_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNUSED SCALING\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "standard_scalar = StandardScaler()\n",
    "x_std = standard_scalar.fit_transform(X_train)\n",
    "print(x_std.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALIZE FEATURE DISTRIBUTIONS OPTIONALLY\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "X_train[X_train.dtypes[(X_train.dtypes==\"float64\")|(X_train.dtypes==\"int64\")]\n",
    "                        .index.values].hist(figsize=[11,11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8203, 12)\n",
      "(8203,)\n"
     ]
    }
   ],
   "source": [
    "# IT COULDN'T HURT TO SEE THE FINAL SHAPES\n",
    "\n",
    "# print(X_train.describe())\n",
    "# print(X_train.head())\n",
    "# print(y_train.describe())\n",
    "# print(y_train.head())\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "regr = RandomForestRegressor(n_estimators=10, max_depth=20, random_state=0, verbose=1)\n",
    "regr.fit(X_train, y_train)\n",
    "preds = regr.predict(X_test)\n",
    "\n",
    "errors = np.absolute(((preds - y_test) / y_test) * 100)\n",
    "# erros = \n",
    "print(errors)\n",
    "print(np.mean(errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET OUTPUT OF FEATURE IMPORTANCE\n",
    "\n",
    "feature_importances = regr.feature_importances_\n",
    "feature_importances = pd.Series(feature_importances)\n",
    "feature_importance_df = pd.DataFrame({'feature': X_train.columns,'feature_importance': feature_importances})\n",
    "feature_importance_df = feature_importance_df.sort_values(by=['feature_importance'])\n",
    "for index, row in feature_importance_df.iterrows():\n",
    "    print(row['feature'], 'has importance: ', row['feature_importance'])\n",
    "# for feature_importance in regr.feature_importances_:\n",
    "    \n",
    "print(errors.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS WILL ONLY WORK WITH THE BUCKET METHOD\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "regr = RandomForestClassifier(n_estimators=10, max_depth=20, random_state=0, verbose=1)\n",
    "regr.fit(X_train, y_train)\n",
    "preds = regr.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(accuracy_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "classifiers = [\n",
    "    svm.SVR(),\n",
    "    MLPRegressor(solver='lbfgs', alpha=1e-5,\n",
    "                     hidden_layer_sizes=(5, 2), random_state=1),\n",
    "    linear_model.SGDRegressor(),\n",
    "    linear_model.BayesianRidge(),\n",
    "    linear_model.LassoLars(),\n",
    "    linear_model.ARDRegression(),\n",
    "#     linear_model.ARDRegression(),\n",
    "    linear_model.PassiveAggressiveRegressor(),\n",
    "    linear_model.TheilSenRegressor(),\n",
    "    linear_model.LinearRegression()]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# estimator = svm.SVR(kernel=\"linear\")\n",
    "\n",
    "# selector = RFECV(estimator, step=1, cv=5, verbose=1)\n",
    "# selector = selector.fit(X_train, y_train)\n",
    "# selector.support_ \n",
    "# # array([ True,  True,  True,  True,  True,\n",
    "# #         False, False, False, False, False], dtype=bool)\n",
    "# selector.ranking_\n",
    "# # array([1, 1, 1, 1, 1, 6, 4, 3, 2, 5])\n",
    "\n",
    "\n",
    "#     print(np.sum(preds - y_test))\n",
    "#     print(clf.predict(X_test),'\\n')\n",
    "#     print(y_test)\n",
    "#     print('accuracy score:', accuracy_score(y_test, clf.predict(X_test)), '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in classifiers:\n",
    "    print(item)\n",
    "    clf = item\n",
    "    clf.fit(X_train, y_train)\n",
    "    preds = clf.predict(X_test)\n",
    "    errors = np.absolute(((preds - y_test) / y_test) * 100)\n",
    "#     print(errors)\n",
    "    print(np.mean(errors))\n",
    "    print(errors.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn import tree\n",
    "classifier = tree.DecisionTreeClassifier()\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "classifiers = [\n",
    "#     KNeighborsClassifier(3),\n",
    "#     SVC(kernel=\"linear\", C=0.025),\n",
    "    SVC(gamma=2, C=1),\n",
    "#     GaussianProcessClassifier(1.0 * RBF(1.0)),\n",
    "#     DecisionTreeClassifier(max_depth=5),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "    MLPClassifier(alpha=1),\n",
    "    AdaBoostClassifier(),\n",
    "    GaussianNB(),\n",
    "    QuadraticDiscriminantAnalysis()]\n",
    "from sklearn.metrics import accuracy_score\n",
    "for item in classifiers:\n",
    "    print(item)\n",
    "    clf = item\n",
    "    clf.fit(scale(X_train), y_train)\n",
    "    preds = clf.predict(scale(X_test))\n",
    "    print(accuracy_score(y_test, preds))\n",
    "#     errors = np.absolute(((preds - y_test) / y_test) * 100)\n",
    "#     print(errors)\n",
    "#     print(np.mean(errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scale(X_train).head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(potential_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "potential_X = X_train.values\n",
    "\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(potential_X)\n",
    "potential_y = y\n",
    "df2 = pd.DataFrame(x_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df2)\n",
    "print(x_scaled)\n",
    "print(potential_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn import datasets\n",
    "import numpy\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "# load pima price dataset\n",
    "dataset = datasets.load_boston()\n",
    "# split into input (X) and output (Y) variables\n",
    "# X = dataset[:,0:8]\n",
    "# Y = dataset[:,8]\n",
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=12, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.add(Dense(1, activation='linear'))\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "# model.fit(X, y, epochs=1000, verbose=0)\n",
    "\n",
    "# Compile model\n",
    "# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# Fit the model\n",
    "model.fit(df2, y_train, epochs=150, batch_size=10)\n",
    "# evaluate the model\n",
    "scores = model.evaluate(X_train, y_train)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import read_csv\n",
    "import math\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Flatten\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# # convert an array of values into a dataset matrix\n",
    "# def create_dataset(dataset, look_back=1):\n",
    "# \tdataX, dataY = [], []\n",
    "# \tfor i in range(len(dataset)-look_back-1):\n",
    "# \t\ta = dataset[i:(i+look_back), 0]\n",
    "# \t\tdataX.append(a)\n",
    "# \t\tdataY.append(dataset[i + look_back, 0])\n",
    "# \treturn numpy.array(dataX), numpy.array(dataY)\n",
    "# # fix random seed for reproducibility\n",
    "# numpy.random.seed(7)\n",
    "# # load the dataset\n",
    "# # dataframe = read_csv('Syngenta/Syngenta_2017/Experiment_dataset.csv', engine='python', skipfooter=3)\n",
    "# dataset = dataframe.values\n",
    "# dataset = dataset.astype('float32')\n",
    "# # normalize the dataset\n",
    "# scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "# dataset = scaler.fit_transform(dataset)\n",
    "# # split into train and test sets\n",
    "# train_size = int(len(dataset) * 0.67)\n",
    "# test_size = len(dataset) - train_size\n",
    "# train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n",
    "# # reshape into X=t and Y=t+1\n",
    "look_back = 12\n",
    "# trainX, trainY = create_dataset(train, look_back)\n",
    "# testX, testY = create_dataset(test, look_back)\n",
    "# # reshape input to be [samples, time steps, features]\n",
    "# trainX = numpy.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
    "# testX = numpy.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n",
    "# create and fit the LSTM network\n",
    "model = Sequential()\n",
    "model.add(LSTM(4, input_shape=(8203, look_back)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=1, verbose=2)\n",
    "# make predictions\n",
    "trainPredict = model.predict(X_train)\n",
    "testPredict = model.predict(X_test)\n",
    "# invert predictions\n",
    "trainPredict = scaler.inverse_transform(trainPredict)\n",
    "trainY = scaler.inverse_transform([trainY])\n",
    "testPredict = scaler.inverse_transform(testPredict)\n",
    "testY = scaler.inverse_transform([testY])\n",
    "# calculate root mean squared error\n",
    "trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\n",
    "print('Train Score: %.2f RMSE' % (trainScore))\n",
    "testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\n",
    "print('Test Score: %.2f RMSE' % (testScore))\n",
    "# shift train predictions for plotting\n",
    "trainPredictPlot = numpy.empty_like(dataset)\n",
    "trainPredictPlot[:, :] = numpy.nan\n",
    "trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n",
    "# shift test predictions for plotting\n",
    "testPredictPlot = numpy.empty_like(dataset)\n",
    "testPredictPlot[:, :] = numpy.nan\n",
    "testPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict\n",
    "# plot baseline and predictions\n",
    "plt.plot(scaler.inverse_transform(dataset))\n",
    "plt.plot(trainPredictPlot)\n",
    "plt.plot(testPredictPlot)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
